{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3617a8",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9fbc9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "xls = pd.ExcelFile('Judgment4.xlsx')\n",
    "df_screen = pd.read_excel(xls, 'Screen')\n",
    "df_a = pd.read_excel(xls, 'a')\n",
    "df_b = pd.read_excel(xls, 'b')\n",
    "df_c = pd.read_excel(xls, 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4908e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df_a.dropna()\n",
    "df_b = df_b.dropna()\n",
    "df_c = df_c.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c57754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a['fldMaR'] = df_a['fldMaR'].apply(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c98511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nh/bqvjwbm11q39ckgjc3_j5_h80000gn/T/ipykernel_45836/855549382.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_a['fldMaR'] = df_a['fldMaR'].str.replace(\"\\(p\\)\", \",\")\n",
      "/var/folders/nh/bqvjwbm11q39ckgjc3_j5_h80000gn/T/ipykernel_45836/855549382.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_a['fldMaR'] = df_a['fldMaR'].str.replace(\"\\(q\\)\", \",\")\n",
      "/var/folders/nh/bqvjwbm11q39ckgjc3_j5_h80000gn/T/ipykernel_45836/855549382.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_a['fldMaR'] = df_a['fldMaR'].str.replace(\"\\(\\)\", \",\")\n"
     ]
    }
   ],
   "source": [
    "df_a['fldMaR'] = df_a['fldMaR'].str.replace(\"\\(p\\)\", \",\")\n",
    "df_a['fldMaR'] = df_a['fldMaR'].str.replace(\"\\(q\\)\", \",\")\n",
    "df_a['fldMaR'] = df_a['fldMaR'].str.replace(\" q \", \",\")\n",
    "df_a['fldMaR'] = df_a['fldMaR'].str.replace(\" p \", \",\")\n",
    "df_a['fldMaR'] = df_a['fldMaR'].str.replace(\"\\(\\)\", \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bc44b3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fldMaR</th>\n",
       "      <th>fldMaS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>call and apologize</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>make a phone call</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atempt to call and say you would be late.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>try to call.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>call someone and tell them i would be late.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>i would be frantic,i'd get up and get my cloth...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>call and tell them i'll be late.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>i would go into a panic. get up and either tel...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>call the place and see if i can reschedule or ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>i'd pick up the phone and call them and tell t...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>957 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                fldMaR  fldMaS\n",
       "0                                   call and apologize     2.0\n",
       "1                                    make a phone call     2.0\n",
       "2            atempt to call and say you would be late.     2.0\n",
       "3                                         try to call.     2.0\n",
       "4          call someone and tell them i would be late.     2.0\n",
       "..                                                 ...     ...\n",
       "952  i would be frantic,i'd get up and get my cloth...     1.0\n",
       "953                  call and tell them i'll be late.      2.0\n",
       "954  i would go into a panic. get up and either tel...     2.0\n",
       "955  call the place and see if i can reschedule or ...     2.0\n",
       "956  i'd pick up the phone and call them and tell t...     2.0\n",
       "\n",
       "[957 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a716a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_a['fldMaR'], df_a['fldMaS'], test_size = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "45c13149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70                         hurry around and get ready , \n",
       "550    call them up and tell them i forgot and will b...\n",
       "175                                     call,say be late\n",
       "542    make a phone call and beg for forgiveness that...\n",
       "305                                   call and tell them\n",
       "                             ...                        \n",
       "851    i would immediately call who i had the appt wi...\n",
       "315                        call and say going to be late\n",
       "955    call the place and see if i can reschedule or ...\n",
       "113    i would call them and cancel or tell them that...\n",
       "341                   call and tell that i had a problem\n",
       "Name: fldMaR, Length: 909, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3eb0d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41fba53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fldMaR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>hurry around and get ready ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>call them up and tell them i forgot and will b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>call,say be late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>make a phone call and beg for forgiveness that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>call and tell them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>i would immediately call who i had the appt wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>call and say going to be late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>call the place and see if i can reschedule or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>i would call them and cancel or tell them that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>call and tell that i had a problem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                fldMaR\n",
       "70                       hurry around and get ready , \n",
       "550  call them up and tell them i forgot and will b...\n",
       "175                                   call,say be late\n",
       "542  make a phone call and beg for forgiveness that...\n",
       "305                                 call and tell them\n",
       "..                                                 ...\n",
       "851  i would immediately call who i had the appt wi...\n",
       "315                      call and say going to be late\n",
       "955  call the place and see if i can reschedule or ...\n",
       "113  i would call them and cancel or tell them that...\n",
       "341                 call and tell that i had a problem\n",
       "\n",
       "[909 rows x 1 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e5b0887a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fldMaS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fldMaS\n",
       "70      1.0\n",
       "550     2.0\n",
       "175     2.0\n",
       "542     2.0\n",
       "305     2.0\n",
       "..      ...\n",
       "851     2.0\n",
       "315     2.0\n",
       "955     2.0\n",
       "113     2.0\n",
       "341     1.0\n",
       "\n",
       "[909 rows x 1 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e1d95026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_backup, X_test_backup, y_train_back_up, y_test_backup = X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "359d366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = X_train_backup, X_test_backup, y_train_back_up, y_test_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ec0aeccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sentence_transformers import SentenceTransformer\n",
    "#bert_model = SentenceTransformer('saved_models/bert.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6eb337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    # Import our models. The package will take care of downloading the models automatically\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-roberta-large\")\n",
    "    model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-roberta-large\")\n",
    "\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2a30919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(\"hi\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "22b27061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sentence_transformers import SentenceTransformer\n",
    "#bert_model2 = SentenceTransformer('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3006f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_hub as hub\n",
    "\n",
    "#embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "db20733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_X_train = X_train['fldMaR'].tolist()\n",
    "#X_train_embedding_tensor = bert_model.encode(lst_X_train, convert_to_tensor=True)\n",
    "X_train_embedding_tensor = embed(lst_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "73224186",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = []\n",
    "\n",
    "for e in X_train_embedding_tensor:\n",
    "    X_train_embeddings.append(e.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f6e67472",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train_embeddings = pd.DataFrame(columns=['fldMaR'])\n",
    "df_X_train_embeddings['fldMaR'] = X_train_embedding_tensor.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7790b60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fldMaR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.3672241270542145, -0.896980881690979, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.40602999925613403, -0.8170621395111084, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.478663831949234, -0.28665512800216675, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.4598325788974762, -0.4606322944164276, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.381252259016037, -0.5550910830497742, 0.55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>[-0.14179180562496185, 0.29456406831741333, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>[-0.2911086082458496, -0.1730719804763794, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>[0.14192607998847961, -0.23637710511684418, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>[-0.11157785356044769, 0.11203710734844208, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>[-0.5276868343353271, -0.5845310091972351, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                fldMaR\n",
       "0    [-0.3672241270542145, -0.896980881690979, 0.04...\n",
       "1    [-0.40602999925613403, -0.8170621395111084, -0...\n",
       "2    [-0.478663831949234, -0.28665512800216675, -0....\n",
       "3    [-0.4598325788974762, -0.4606322944164276, -0....\n",
       "4    [-0.381252259016037, -0.5550910830497742, 0.55...\n",
       "..                                                 ...\n",
       "904  [-0.14179180562496185, 0.29456406831741333, 0....\n",
       "905  [-0.2911086082458496, -0.1730719804763794, -0....\n",
       "906  [0.14192607998847961, -0.23637710511684418, 0....\n",
       "907  [-0.11157785356044769, 0.11203710734844208, -0...\n",
       "908  [-0.5276868343353271, -0.5845310091972351, -0....\n",
       "\n",
       "[909 rows x 1 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_train_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "54c36aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_embeddings = []\n",
    "#for e in X_train_embedding_tensor:\n",
    "#    X_train_embeddings.append(list(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5a88250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_X_train_embeddings = pd.DataFrame(out, columns=[\"fldSResponse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "834dc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_X_test = X_test['fldMaR'].tolist()\n",
    "#X_test_embedding_tensor = bert_model.encode(lst_X_test, convert_to_tensor=True)\n",
    "X_test_embedding_tensor = embed(lst_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b0046767",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_embeddings = []\n",
    "\n",
    "for e in X_test_embedding_tensor:\n",
    "    X_test_embeddings.append(e.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "feef2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_test_embeddings = pd.DataFrame(columns=['fldMaR'])\n",
    "df_X_test_embeddings['fldMaR'] = X_test_embedding_tensor.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "54b77033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fldMaR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.2893258333206177, -0.013050932437181473, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.5309541821479797, -0.6432598233222961, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.7075684070587158, -0.5955490469932556, 0.14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.6947242617607117, -0.781536340713501, 0.54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.06241545081138611, -0.3602924048900604, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.077024444937706, -0.5150719285011292, -0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-0.5280616879463196, -0.6082912683486938, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-0.5890246033668518, -0.6096842885017395, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.014528116211295128, -0.14018568396568298, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[-0.650503396987915, 0.12910747528076172, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[-0.5155525207519531, -0.32125991582870483, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[-0.20488576591014862, 0.0011807674309238791, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[-0.1199881061911583, 0.17025358974933624, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[-0.7064764499664307, -0.36528658866882324, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[-0.10300345718860626, -0.40054070949554443, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[-0.5743787884712219, -0.48367685079574585, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[-0.38494136929512024, -0.3316045105457306, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[-0.33091217279434204, -0.34495946764945984, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[-0.27574437856674194, -0.3046414852142334, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[-0.04163500294089317, 0.09777173399925232, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[-0.34416961669921875, -0.5696716904640198, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.21776904165744781, -0.30680179595947266, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[-0.24169202148914337, -0.49088895320892334, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.4443370997905731, -0.4690437316894531, 0.25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.20819607377052307, 0.22933702170848846, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[-0.10300345718860626, -0.40054070949554443, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.18414252996444702, 0.25886815786361694, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.3987139165401459, -0.838767945766449, 0.169...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.16284136474132538, 0.5070980191230774, -0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[-0.21436835825443268, -0.2874368727207184, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[-0.2905639111995697, -0.3475596308708191, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[-0.07355444133281708, 0.00316815497353673, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.17986048758029938, -0.4049622416496277, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[-0.08594678342342377, 0.06993686407804489, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.06933644413948059, -0.1318885087966919, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[-0.5909005999565125, -0.03289603069424629, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[-0.2785152196884155, -0.3170015513896942, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.04243481531739235, -0.34866324067115784, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[-0.17620375752449036, -0.4690238833427429, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.06997228413820267, -0.6961566805839539, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.37400102615356445, -0.5163190960884094, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[-0.21807917952537537, -0.058579154312610626, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.34701088070869446, -0.4596315026283264, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.3038756847381592, -0.18808822333812714, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.07499533146619797, -0.6280648112297058, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.042233534157276154, -0.22701583802700043, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[-0.7514935731887817, 0.06416641920804977, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[-0.2364790290594101, -0.6912441849708557, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fldMaR\n",
       "0   [-0.2893258333206177, -0.013050932437181473, 0...\n",
       "1   [-0.5309541821479797, -0.6432598233222961, 0.1...\n",
       "2   [0.7075684070587158, -0.5955490469932556, 0.14...\n",
       "3   [-0.6947242617607117, -0.781536340713501, 0.54...\n",
       "4   [-0.06241545081138611, -0.3602924048900604, -0...\n",
       "5   [-0.077024444937706, -0.5150719285011292, -0.5...\n",
       "6   [-0.5280616879463196, -0.6082912683486938, -0....\n",
       "7   [-0.5890246033668518, -0.6096842885017395, 0.1...\n",
       "8   [0.014528116211295128, -0.14018568396568298, -...\n",
       "9   [-0.650503396987915, 0.12910747528076172, -0.1...\n",
       "10  [-0.5155525207519531, -0.32125991582870483, -0...\n",
       "11  [-0.20488576591014862, 0.0011807674309238791, ...\n",
       "12  [-0.1199881061911583, 0.17025358974933624, -0....\n",
       "13  [-0.7064764499664307, -0.36528658866882324, -0...\n",
       "14  [-0.10300345718860626, -0.40054070949554443, -...\n",
       "15  [-0.5743787884712219, -0.48367685079574585, -0...\n",
       "16  [-0.38494136929512024, -0.3316045105457306, -0...\n",
       "17  [-0.33091217279434204, -0.34495946764945984, -...\n",
       "18  [-0.27574437856674194, -0.3046414852142334, -0...\n",
       "19  [-0.04163500294089317, 0.09777173399925232, -0...\n",
       "20  [-0.34416961669921875, -0.5696716904640198, 0....\n",
       "21  [0.21776904165744781, -0.30680179595947266, 0....\n",
       "22  [-0.24169202148914337, -0.49088895320892334, -...\n",
       "23  [0.4443370997905731, -0.4690437316894531, 0.25...\n",
       "24  [0.20819607377052307, 0.22933702170848846, -0....\n",
       "25  [-0.10300345718860626, -0.40054070949554443, -...\n",
       "26  [0.18414252996444702, 0.25886815786361694, 0.2...\n",
       "27  [0.3987139165401459, -0.838767945766449, 0.169...\n",
       "28  [0.16284136474132538, 0.5070980191230774, -0.5...\n",
       "29  [-0.21436835825443268, -0.2874368727207184, -0...\n",
       "30  [-0.2905639111995697, -0.3475596308708191, -0....\n",
       "31  [-0.07355444133281708, 0.00316815497353673, -0...\n",
       "32  [0.17986048758029938, -0.4049622416496277, 0.1...\n",
       "33  [-0.08594678342342377, 0.06993686407804489, 0....\n",
       "34  [0.06933644413948059, -0.1318885087966919, -0....\n",
       "35  [-0.5909005999565125, -0.03289603069424629, 0....\n",
       "36  [-0.2785152196884155, -0.3170015513896942, -0....\n",
       "37  [0.04243481531739235, -0.34866324067115784, 0....\n",
       "38  [-0.17620375752449036, -0.4690238833427429, -0...\n",
       "39  [0.06997228413820267, -0.6961566805839539, -0....\n",
       "40  [0.37400102615356445, -0.5163190960884094, 0.1...\n",
       "41  [-0.21807917952537537, -0.058579154312610626, ...\n",
       "42  [0.34701088070869446, -0.4596315026283264, -0....\n",
       "43  [0.3038756847381592, -0.18808822333812714, -0....\n",
       "44  [0.07499533146619797, -0.6280648112297058, -0....\n",
       "45  [0.042233534157276154, -0.22701583802700043, 0...\n",
       "46  [-0.7514935731887817, 0.06416641920804977, -0....\n",
       "47  [-0.2364790290594101, -0.6912441849708557, 0.2..."
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "67217a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_embeddings = []\n",
    "#for e in X_test_embedding_tensor:\n",
    "#    X_test_embeddings.append(list(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0ed53bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.tolist()\n",
    "y_test = y_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0d469dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings_backup, X_test_embeddings_backup = X_train_embeddings, X_test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f5f74667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_embeddings_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f8c4a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings_backup = pd.DataFrame(X_train_embeddings_backup)\n",
    "X_test_embeddings_backup = pd.DataFrame(X_test_embeddings_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1c2674e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ae8623f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = np.array(X_train_embeddings)\n",
    "X_test_embeddings = np.array(X_test_embeddings)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f1fa8721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de0620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "633a857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.7305 - val_loss: 0.5158 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51576, saving model to ./tmp/checkpoint_bahar_a5\n",
      "INFO:tensorflow:Assets written to: ./tmp/checkpoint_bahar_a5/assets\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.5407 - accuracy: 0.7932 - val_loss: 0.5332 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51576\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4956 - accuracy: 0.8020 - val_loss: 0.5051 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51576 to 0.50508, saving model to ./tmp/checkpoint_bahar_a5\n",
      "INFO:tensorflow:Assets written to: ./tmp/checkpoint_bahar_a5/assets\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4445 - accuracy: 0.8361 - val_loss: 0.5189 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.50508\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.8427 - val_loss: 0.4570 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50508 to 0.45701, saving model to ./tmp/checkpoint_bahar_a5\n",
      "INFO:tensorflow:Assets written to: ./tmp/checkpoint_bahar_a5/assets\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.3887 - accuracy: 0.8493 - val_loss: 0.5510 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.45701\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.3619 - accuracy: 0.8680 - val_loss: 0.5238 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.45701\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8812 - val_loss: 0.5502 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.45701\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.3198 - accuracy: 0.8889 - val_loss: 0.5863 - val_accuracy: 0.7292\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.45701\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.2864 - accuracy: 0.9032 - val_loss: 0.5778 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.45701\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.2852 - accuracy: 0.8944 - val_loss: 0.5025 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.45701\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.9109 - val_loss: 0.5318 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.45701\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8977 - val_loss: 0.5447 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.45701\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 0.9164 - val_loss: 0.5634 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.45701\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.2166 - accuracy: 0.9351 - val_loss: 0.6077 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.45701\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.9274 - val_loss: 0.6576 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.45701\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1887 - accuracy: 0.9351 - val_loss: 0.5868 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.45701\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1893 - accuracy: 0.9428 - val_loss: 0.5838 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.45701\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1612 - accuracy: 0.9538 - val_loss: 0.5819 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.45701\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.9516 - val_loss: 0.5615 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.45701\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1543 - accuracy: 0.9461 - val_loss: 0.5921 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.45701\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1623 - accuracy: 0.9483 - val_loss: 0.5490 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.45701\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.9549 - val_loss: 0.5904 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.45701\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1551 - accuracy: 0.9461 - val_loss: 0.5981 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.45701\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1410 - accuracy: 0.9549 - val_loss: 0.6198 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.45701\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1243 - accuracy: 0.9626 - val_loss: 0.6464 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.45701\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1053 - accuracy: 0.9659 - val_loss: 0.5455 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.45701\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1113 - accuracy: 0.9571 - val_loss: 0.6556 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.45701\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1136 - accuracy: 0.9637 - val_loss: 0.6320 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.45701\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1024 - accuracy: 0.9670 - val_loss: 0.6559 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.45701\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1046 - accuracy: 0.9648 - val_loss: 0.7095 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.45701\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0997 - accuracy: 0.9670 - val_loss: 0.6557 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.45701\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0967 - accuracy: 0.9659 - val_loss: 0.7952 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.45701\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1035 - accuracy: 0.9648 - val_loss: 0.7352 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.45701\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9725 - val_loss: 0.7067 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.45701\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0841 - accuracy: 0.9725 - val_loss: 0.6709 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.45701\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.9714 - val_loss: 0.7545 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.45701\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9791 - val_loss: 0.6851 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.45701\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0856 - accuracy: 0.9725 - val_loss: 0.6460 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.45701\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.9681 - val_loss: 0.7799 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.45701\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0917 - accuracy: 0.9714 - val_loss: 0.7886 - val_accuracy: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: val_loss did not improve from 0.45701\n",
      "Epoch 42/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.9758 - val_loss: 0.8352 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.45701\n",
      "Epoch 43/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0919 - accuracy: 0.9681 - val_loss: 0.7835 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.45701\n",
      "Epoch 44/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.9769 - val_loss: 0.8424 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.45701\n",
      "Epoch 45/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0720 - accuracy: 0.9813 - val_loss: 0.7200 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.45701\n",
      "Epoch 46/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9692 - val_loss: 0.8330 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.45701\n",
      "Epoch 47/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9791 - val_loss: 0.8443 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.45701\n",
      "Epoch 48/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.9736 - val_loss: 0.7738 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.45701\n",
      "Epoch 49/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.9758 - val_loss: 0.6605 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.45701\n",
      "Epoch 50/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0766 - accuracy: 0.9802 - val_loss: 0.7820 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.45701\n",
      "Epoch 51/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9780 - val_loss: 0.9422 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.45701\n",
      "Epoch 52/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9780 - val_loss: 0.7738 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.45701\n",
      "Epoch 53/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9824 - val_loss: 0.8331 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.45701\n",
      "Epoch 54/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9747 - val_loss: 0.9267 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.45701\n",
      "Epoch 55/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9857 - val_loss: 0.9502 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.45701\n",
      "Epoch 56/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9725 - val_loss: 0.9257 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.45701\n",
      "Epoch 57/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9670 - val_loss: 0.9016 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.45701\n",
      "Epoch 58/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9791 - val_loss: 0.8686 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.45701\n",
      "Epoch 59/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.9846 - val_loss: 0.9414 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.45701\n",
      "Epoch 60/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9846 - val_loss: 0.8952 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.45701\n",
      "Epoch 61/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9868 - val_loss: 0.8339 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.45701\n",
      "Epoch 62/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0353 - accuracy: 0.9923 - val_loss: 1.0716 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.45701\n",
      "Epoch 63/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9780 - val_loss: 0.8980 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.45701\n",
      "Epoch 64/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9846 - val_loss: 1.1653 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.45701\n",
      "Epoch 65/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0454 - accuracy: 0.9835 - val_loss: 0.9652 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.45701\n",
      "Epoch 66/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9780 - val_loss: 0.9649 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.45701\n",
      "Epoch 67/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9758 - val_loss: 0.9384 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.45701\n",
      "Epoch 68/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9725 - val_loss: 1.1512 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.45701\n",
      "Epoch 69/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9725 - val_loss: 1.0476 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.45701\n",
      "Epoch 70/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9747 - val_loss: 1.0836 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.45701\n",
      "Epoch 71/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.9835 - val_loss: 0.9847 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.45701\n",
      "Epoch 72/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9813 - val_loss: 1.0624 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.45701\n",
      "Epoch 73/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0403 - accuracy: 0.9835 - val_loss: 1.0189 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.45701\n",
      "Epoch 74/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9879 - val_loss: 1.0381 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.45701\n",
      "Epoch 75/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0335 - accuracy: 0.9901 - val_loss: 1.1360 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.45701\n",
      "Epoch 76/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9824 - val_loss: 1.0757 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.45701\n",
      "Epoch 77/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9835 - val_loss: 1.0935 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.45701\n",
      "Epoch 78/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0402 - accuracy: 0.9835 - val_loss: 1.0922 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.45701\n",
      "Epoch 79/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0410 - accuracy: 0.9846 - val_loss: 1.1670 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.45701\n",
      "Epoch 80/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.9824 - val_loss: 1.2458 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.45701\n",
      "Epoch 81/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0432 - accuracy: 0.9857 - val_loss: 1.1940 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.45701\n",
      "Epoch 82/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 0.9835 - val_loss: 1.1461 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.45701\n",
      "Epoch 83/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9868 - val_loss: 1.3195 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.45701\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0456 - accuracy: 0.9824 - val_loss: 1.1197 - val_accuracy: 0.8542\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.45701\n",
      "Epoch 85/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9824 - val_loss: 1.0476 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.45701\n",
      "Epoch 86/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0450 - accuracy: 0.9835 - val_loss: 1.0819 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.45701\n",
      "Epoch 87/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9780 - val_loss: 1.1160 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.45701\n",
      "Epoch 88/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0412 - accuracy: 0.9835 - val_loss: 1.1046 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.45701\n",
      "Epoch 89/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.9868 - val_loss: 1.1976 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.45701\n",
      "Epoch 90/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0353 - accuracy: 0.9912 - val_loss: 1.3145 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.45701\n",
      "Epoch 91/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.9901 - val_loss: 1.3936 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.45701\n",
      "Epoch 92/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0373 - accuracy: 0.9791 - val_loss: 1.1942 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.45701\n",
      "Epoch 93/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0409 - accuracy: 0.9813 - val_loss: 1.1623 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.45701\n",
      "Epoch 94/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9846 - val_loss: 1.1788 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.45701\n",
      "Epoch 95/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0330 - accuracy: 0.9868 - val_loss: 1.3963 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.45701\n",
      "Epoch 96/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0416 - accuracy: 0.9824 - val_loss: 1.1566 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.45701\n",
      "Epoch 97/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0408 - accuracy: 0.9813 - val_loss: 1.3406 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.45701\n",
      "Epoch 98/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9736 - val_loss: 1.1916 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.45701\n",
      "Epoch 99/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9758 - val_loss: 1.1929 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.45701\n",
      "Epoch 100/100\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9747 - val_loss: 1.3911 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.45701\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.7730 - accuracy: 0.6964 - val_loss: 0.5422 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.45701\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5644 - accuracy: 0.8031 - val_loss: 0.5053 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.45701\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5160 - accuracy: 0.8020 - val_loss: 0.4963 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.45701\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4889 - accuracy: 0.8229 - val_loss: 0.4840 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.45701\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4494 - accuracy: 0.8328 - val_loss: 0.4895 - val_accuracy: 0.8542\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.45701\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4160 - accuracy: 0.8460 - val_loss: 0.4658 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.45701\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4010 - accuracy: 0.8636 - val_loss: 0.4528 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.45701 to 0.45283, saving model to ./tmp/checkpoint_bahar_a5\n",
      "INFO:tensorflow:Assets written to: ./tmp/checkpoint_bahar_a5/assets\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8592 - val_loss: 0.4574 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.45283\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.3623 - accuracy: 0.8658 - val_loss: 0.4655 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.45283\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.3505 - accuracy: 0.8768 - val_loss: 0.4662 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.45283\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.3258 - accuracy: 0.8900 - val_loss: 0.4891 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.45283\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2935 - accuracy: 0.8999 - val_loss: 0.4652 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.45283\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2775 - accuracy: 0.8977 - val_loss: 0.5036 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.45283\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2637 - accuracy: 0.9186 - val_loss: 0.4778 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.45283\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2472 - accuracy: 0.9230 - val_loss: 0.5495 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.45283\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2535 - accuracy: 0.9087 - val_loss: 0.4969 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.45283\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2661 - accuracy: 0.9065 - val_loss: 0.5225 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.45283\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2147 - accuracy: 0.9340 - val_loss: 0.5107 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.45283\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2195 - accuracy: 0.9230 - val_loss: 0.5411 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.45283\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1935 - accuracy: 0.9362 - val_loss: 0.5285 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.45283\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1703 - accuracy: 0.9494 - val_loss: 0.5522 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.45283\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1502 - accuracy: 0.9494 - val_loss: 0.5534 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.45283\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1718 - accuracy: 0.9373 - val_loss: 0.5559 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.45283\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1573 - accuracy: 0.9516 - val_loss: 0.5522 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.45283\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1482 - accuracy: 0.9571 - val_loss: 0.6264 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.45283\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1357 - accuracy: 0.9582 - val_loss: 0.5904 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.45283\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1322 - accuracy: 0.9604 - val_loss: 0.6214 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.45283\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1193 - accuracy: 0.9747 - val_loss: 0.6075 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.45283\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9582 - val_loss: 0.7273 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.45283\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1066 - accuracy: 0.9714 - val_loss: 0.6483 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.45283\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1145 - accuracy: 0.9692 - val_loss: 0.6444 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.45283\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1116 - accuracy: 0.9670 - val_loss: 0.6538 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.45283\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1121 - accuracy: 0.9692 - val_loss: 0.6823 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.45283\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.9813 - val_loss: 0.7339 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.45283\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1106 - accuracy: 0.9626 - val_loss: 0.6802 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.45283\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9736 - val_loss: 0.7642 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.45283\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0957 - accuracy: 0.9725 - val_loss: 0.7077 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.45283\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9703 - val_loss: 0.7333 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.45283\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0830 - accuracy: 0.9802 - val_loss: 0.7412 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.45283\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0916 - accuracy: 0.9758 - val_loss: 0.6162 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.45283\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0753 - accuracy: 0.9835 - val_loss: 0.6822 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.45283\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.9758 - val_loss: 0.7061 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.45283\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.9736 - val_loss: 0.6525 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.45283\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.9802 - val_loss: 0.7172 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.45283\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0762 - accuracy: 0.9714 - val_loss: 0.7230 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.45283\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0828 - accuracy: 0.9758 - val_loss: 0.7779 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.45283\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0726 - accuracy: 0.9747 - val_loss: 0.8255 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.45283\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9802 - val_loss: 0.7320 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.45283\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.9758 - val_loss: 0.9115 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.45283\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.9780 - val_loss: 0.8134 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.45283\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0510 - accuracy: 0.9857 - val_loss: 0.8321 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.45283\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0609 - accuracy: 0.9835 - val_loss: 0.9082 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.45283\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.9791 - val_loss: 0.9265 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.45283\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9769 - val_loss: 0.9719 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.45283\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.9791 - val_loss: 0.9430 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.45283\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0624 - accuracy: 0.9802 - val_loss: 0.8947 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.45283\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9835 - val_loss: 0.8721 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.45283\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0545 - accuracy: 0.9824 - val_loss: 0.8474 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.45283\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.9703 - val_loss: 0.8854 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.45283\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0600 - accuracy: 0.9747 - val_loss: 1.0212 - val_accuracy: 0.7083\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.45283\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9791 - val_loss: 0.9614 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.45283\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 0.9780 - val_loss: 0.9595 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.45283\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0624 - accuracy: 0.9813 - val_loss: 1.0004 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.45283\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9813 - val_loss: 0.9450 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.45283\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0531 - accuracy: 0.9846 - val_loss: 0.9856 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.45283\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0504 - accuracy: 0.9835 - val_loss: 0.9867 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.45283\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0535 - accuracy: 0.9791 - val_loss: 0.9793 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.45283\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0397 - accuracy: 0.9879 - val_loss: 1.0226 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.45283\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0441 - accuracy: 0.9857 - val_loss: 1.0640 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.45283\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0430 - accuracy: 0.9868 - val_loss: 1.0779 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.45283\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0471 - accuracy: 0.9857 - val_loss: 1.0561 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.45283\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0420 - accuracy: 0.9857 - val_loss: 1.0427 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.45283\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0479 - accuracy: 0.9813 - val_loss: 1.0890 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.45283\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0443 - accuracy: 0.9890 - val_loss: 1.0464 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.45283\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0473 - accuracy: 0.9857 - val_loss: 1.0990 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.45283\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0447 - accuracy: 0.9846 - val_loss: 0.9497 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.45283\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 0.9780 - val_loss: 0.9590 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.45283\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0397 - accuracy: 0.9857 - val_loss: 1.0104 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.45283\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0464 - accuracy: 0.9813 - val_loss: 1.0109 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.45283\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.9857 - val_loss: 0.9894 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.45283\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0426 - accuracy: 0.9813 - val_loss: 1.0092 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.45283\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.9890 - val_loss: 1.0039 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.45283\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0512 - accuracy: 0.9824 - val_loss: 0.9964 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.45283\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0493 - accuracy: 0.9813 - val_loss: 1.1337 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.45283\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0465 - accuracy: 0.9824 - val_loss: 1.0942 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.45283\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0489 - accuracy: 0.9835 - val_loss: 1.1808 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.45283\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0307 - accuracy: 0.9901 - val_loss: 1.1518 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.45283\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0302 - accuracy: 0.9912 - val_loss: 1.1884 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.45283\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0313 - accuracy: 0.9901 - val_loss: 1.2804 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.45283\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0450 - accuracy: 0.9824 - val_loss: 1.2195 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.45283\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.9846 - val_loss: 1.2766 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.45283\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.9901 - val_loss: 1.2675 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.45283\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9835 - val_loss: 1.2418 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.45283\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.9890 - val_loss: 1.2725 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.45283\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0250 - accuracy: 0.9912 - val_loss: 1.2760 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.45283\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.9912 - val_loss: 1.2796 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.45283\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0331 - accuracy: 0.9868 - val_loss: 1.2396 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.45283\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0391 - accuracy: 0.9857 - val_loss: 1.2608 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.45283\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9868 - val_loss: 1.3736 - val_accuracy: 0.7292\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.45283\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.9890 - val_loss: 1.2769 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.45283\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.8848 - accuracy: 0.6337 - val_loss: 0.5965 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.45283\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6235 - accuracy: 0.7778 - val_loss: 0.5372 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.45283\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5724 - accuracy: 0.7921 - val_loss: 0.4864 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.45283\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5280 - accuracy: 0.7943 - val_loss: 0.4907 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.45283\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5058 - accuracy: 0.8086 - val_loss: 0.5012 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.45283\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4681 - accuracy: 0.8262 - val_loss: 0.4653 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.45283\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4472 - accuracy: 0.8328 - val_loss: 0.4685 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.45283\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4355 - accuracy: 0.8416 - val_loss: 0.4630 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.45283\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4207 - accuracy: 0.8471 - val_loss: 0.4589 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.45283\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3995 - accuracy: 0.8570 - val_loss: 0.4479 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.45283 to 0.44795, saving model to ./tmp/checkpoint_bahar_a5\n",
      "INFO:tensorflow:Assets written to: ./tmp/checkpoint_bahar_a5/assets\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.3790 - accuracy: 0.8504 - val_loss: 0.4514 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.44795\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3646 - accuracy: 0.8680 - val_loss: 0.4822 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.44795\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3408 - accuracy: 0.8812 - val_loss: 0.4482 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.44795\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3349 - accuracy: 0.8834 - val_loss: 0.4335 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.44795 to 0.43351, saving model to ./tmp/checkpoint_bahar_a5\n",
      "INFO:tensorflow:Assets written to: ./tmp/checkpoint_bahar_a5/assets\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3174 - accuracy: 0.8889 - val_loss: 0.4699 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43351\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2993 - accuracy: 0.9021 - val_loss: 0.4400 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.43351\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2914 - accuracy: 0.9021 - val_loss: 0.4485 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.43351\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2912 - accuracy: 0.9043 - val_loss: 0.4491 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.43351\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2750 - accuracy: 0.9021 - val_loss: 0.4374 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.43351\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2661 - accuracy: 0.9010 - val_loss: 0.4662 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.43351\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2485 - accuracy: 0.9219 - val_loss: 0.4908 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.43351\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2472 - accuracy: 0.9142 - val_loss: 0.5161 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.43351\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2297 - accuracy: 0.9131 - val_loss: 0.5013 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.43351\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2215 - accuracy: 0.9285 - val_loss: 0.4912 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.43351\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2053 - accuracy: 0.9351 - val_loss: 0.4926 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.43351\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2134 - accuracy: 0.9395 - val_loss: 0.5107 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.43351\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1949 - accuracy: 0.9296 - val_loss: 0.5628 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.43351\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1769 - accuracy: 0.9461 - val_loss: 0.5109 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.43351\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1672 - accuracy: 0.9527 - val_loss: 0.5081 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.43351\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1683 - accuracy: 0.9549 - val_loss: 0.5586 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.43351\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1696 - accuracy: 0.9461 - val_loss: 0.5677 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.43351\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1450 - accuracy: 0.9527 - val_loss: 0.5471 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.43351\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9604 - val_loss: 0.4937 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.43351\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1698 - accuracy: 0.9483 - val_loss: 0.5554 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.43351\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1399 - accuracy: 0.9637 - val_loss: 0.5506 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.43351\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1341 - accuracy: 0.9637 - val_loss: 0.5666 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.43351\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1356 - accuracy: 0.9648 - val_loss: 0.5660 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.43351\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1328 - accuracy: 0.9659 - val_loss: 0.5320 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.43351\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1277 - accuracy: 0.9648 - val_loss: 0.6060 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.43351\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1253 - accuracy: 0.9692 - val_loss: 0.6065 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.43351\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1185 - accuracy: 0.9648 - val_loss: 0.5734 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.43351\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1124 - accuracy: 0.9703 - val_loss: 0.6041 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.43351\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1010 - accuracy: 0.9769 - val_loss: 0.6240 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.43351\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1090 - accuracy: 0.9648 - val_loss: 0.5746 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.43351\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0957 - accuracy: 0.9747 - val_loss: 0.6070 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.43351\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1067 - accuracy: 0.9681 - val_loss: 0.5775 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.43351\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1015 - accuracy: 0.9703 - val_loss: 0.6750 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.43351\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0961 - accuracy: 0.9714 - val_loss: 0.6525 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.43351\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0915 - accuracy: 0.9725 - val_loss: 0.6353 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.43351\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1027 - accuracy: 0.9703 - val_loss: 0.6416 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.43351\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0803 - accuracy: 0.9835 - val_loss: 0.6865 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.43351\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0983 - accuracy: 0.9670 - val_loss: 0.6155 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.43351\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9714 - val_loss: 0.6022 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.43351\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0826 - accuracy: 0.9758 - val_loss: 0.6178 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.43351\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0844 - accuracy: 0.9758 - val_loss: 0.7164 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.43351\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0848 - accuracy: 0.9791 - val_loss: 0.6976 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.43351\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0837 - accuracy: 0.9769 - val_loss: 0.6725 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.43351\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0782 - accuracy: 0.9791 - val_loss: 0.6958 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.43351\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0802 - accuracy: 0.9758 - val_loss: 0.6607 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.43351\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0857 - accuracy: 0.9681 - val_loss: 0.7114 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.43351\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0819 - accuracy: 0.9725 - val_loss: 0.7472 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.43351\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 0.9857 - val_loss: 0.7319 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.43351\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 0.9835 - val_loss: 0.7064 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.43351\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0595 - accuracy: 0.9824 - val_loss: 0.7459 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.43351\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0606 - accuracy: 0.9824 - val_loss: 0.7553 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.43351\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9857 - val_loss: 0.8230 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.43351\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0700 - accuracy: 0.9780 - val_loss: 0.7818 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.43351\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0656 - accuracy: 0.9791 - val_loss: 0.8060 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.43351\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0685 - accuracy: 0.9769 - val_loss: 0.8665 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.43351\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0600 - accuracy: 0.9824 - val_loss: 0.8795 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.43351\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0450 - accuracy: 0.9868 - val_loss: 0.8447 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.43351\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0689 - accuracy: 0.9769 - val_loss: 0.7653 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.43351\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0546 - accuracy: 0.9857 - val_loss: 0.7568 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.43351\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0580 - accuracy: 0.9846 - val_loss: 0.7856 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.43351\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0648 - accuracy: 0.9835 - val_loss: 0.8471 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.43351\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0529 - accuracy: 0.9868 - val_loss: 0.9106 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.43351\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0494 - accuracy: 0.9835 - val_loss: 0.8934 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.43351\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0655 - accuracy: 0.9791 - val_loss: 0.7886 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.43351\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0638 - accuracy: 0.9813 - val_loss: 0.9130 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.43351\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0606 - accuracy: 0.9813 - val_loss: 0.8586 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.43351\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0444 - accuracy: 0.9857 - val_loss: 0.8566 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.43351\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0497 - accuracy: 0.9868 - val_loss: 0.8615 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.43351\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0444 - accuracy: 0.9846 - val_loss: 0.8858 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.43351\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0515 - accuracy: 0.9824 - val_loss: 0.8655 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.43351\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0474 - accuracy: 0.9879 - val_loss: 0.9116 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.43351\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0474 - accuracy: 0.9857 - val_loss: 0.8465 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.43351\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0632 - accuracy: 0.9835 - val_loss: 0.8982 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.43351\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0514 - accuracy: 0.9846 - val_loss: 0.9403 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.43351\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.9857 - val_loss: 0.9074 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.43351\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0481 - accuracy: 0.9813 - val_loss: 0.8284 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.43351\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0475 - accuracy: 0.9879 - val_loss: 0.9164 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.43351\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0426 - accuracy: 0.9890 - val_loss: 0.9449 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.43351\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0403 - accuracy: 0.9912 - val_loss: 0.9610 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.43351\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.9835 - val_loss: 0.8673 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.43351\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0427 - accuracy: 0.9868 - val_loss: 0.9053 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.43351\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0398 - accuracy: 0.9868 - val_loss: 0.9561 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.43351\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0343 - accuracy: 0.9868 - val_loss: 1.0026 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.43351\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 0.9890 - val_loss: 0.9364 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.43351\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.9901 - val_loss: 1.0179 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.43351\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0270 - accuracy: 0.9923 - val_loss: 1.0687 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.43351\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.8886 - accuracy: 0.6007 - val_loss: 0.7138 - val_accuracy: 0.6875\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.43351\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6588 - accuracy: 0.7514 - val_loss: 0.5662 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.43351\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6221 - accuracy: 0.7723 - val_loss: 0.5208 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.43351\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5872 - accuracy: 0.7844 - val_loss: 0.5334 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.43351\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5426 - accuracy: 0.7910 - val_loss: 0.5098 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.43351\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5200 - accuracy: 0.8020 - val_loss: 0.5036 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43351\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5055 - accuracy: 0.8086 - val_loss: 0.4959 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43351\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4793 - accuracy: 0.8108 - val_loss: 0.4976 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43351\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4677 - accuracy: 0.8174 - val_loss: 0.4905 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43351\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4504 - accuracy: 0.8295 - val_loss: 0.4779 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43351\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4373 - accuracy: 0.8306 - val_loss: 0.4885 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.43351\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4243 - accuracy: 0.8471 - val_loss: 0.4611 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43351\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4257 - accuracy: 0.8427 - val_loss: 0.4999 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43351\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3962 - accuracy: 0.8570 - val_loss: 0.4596 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43351\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3858 - accuracy: 0.8625 - val_loss: 0.4557 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43351\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3780 - accuracy: 0.8658 - val_loss: 0.4878 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.43351\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3660 - accuracy: 0.8592 - val_loss: 0.4592 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.43351\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3588 - accuracy: 0.8779 - val_loss: 0.4717 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.43351\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3455 - accuracy: 0.8801 - val_loss: 0.4580 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.43351\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3400 - accuracy: 0.8801 - val_loss: 0.4811 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.43351\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3505 - accuracy: 0.8702 - val_loss: 0.4833 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.43351\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3367 - accuracy: 0.8812 - val_loss: 0.4847 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.43351\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3208 - accuracy: 0.8944 - val_loss: 0.4939 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.43351\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2996 - accuracy: 0.8911 - val_loss: 0.5001 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.43351\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3057 - accuracy: 0.8900 - val_loss: 0.5260 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.43351\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3029 - accuracy: 0.8911 - val_loss: 0.5222 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.43351\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2774 - accuracy: 0.9021 - val_loss: 0.5401 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.43351\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2790 - accuracy: 0.9010 - val_loss: 0.4978 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.43351\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2745 - accuracy: 0.9164 - val_loss: 0.5130 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.43351\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2747 - accuracy: 0.9098 - val_loss: 0.4972 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.43351\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2654 - accuracy: 0.9164 - val_loss: 0.5119 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.43351\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2455 - accuracy: 0.9131 - val_loss: 0.5144 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.43351\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2570 - accuracy: 0.9021 - val_loss: 0.4983 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.43351\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2548 - accuracy: 0.9098 - val_loss: 0.5448 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.43351\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2501 - accuracy: 0.9164 - val_loss: 0.4959 - val_accuracy: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss did not improve from 0.43351\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2332 - accuracy: 0.9362 - val_loss: 0.5228 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.43351\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2273 - accuracy: 0.9263 - val_loss: 0.5253 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.43351\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2223 - accuracy: 0.9395 - val_loss: 0.5131 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.43351\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2200 - accuracy: 0.9307 - val_loss: 0.5199 - val_accuracy: 0.7708\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.43351\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2077 - accuracy: 0.9296 - val_loss: 0.5099 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.43351\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2036 - accuracy: 0.9263 - val_loss: 0.5053 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.43351\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1941 - accuracy: 0.9461 - val_loss: 0.5032 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.43351\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2073 - accuracy: 0.9285 - val_loss: 0.5247 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.43351\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1940 - accuracy: 0.9439 - val_loss: 0.5448 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.43351\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1726 - accuracy: 0.9516 - val_loss: 0.5244 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.43351\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1817 - accuracy: 0.9494 - val_loss: 0.5257 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.43351\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1786 - accuracy: 0.9417 - val_loss: 0.5223 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.43351\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1794 - accuracy: 0.9516 - val_loss: 0.5590 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.43351\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1716 - accuracy: 0.9516 - val_loss: 0.5354 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.43351\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1638 - accuracy: 0.9626 - val_loss: 0.5014 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.43351\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1744 - accuracy: 0.9428 - val_loss: 0.5054 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.43351\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1627 - accuracy: 0.9538 - val_loss: 0.5386 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.43351\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1658 - accuracy: 0.9505 - val_loss: 0.5235 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.43351\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1709 - accuracy: 0.9549 - val_loss: 0.5571 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.43351\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1672 - accuracy: 0.9417 - val_loss: 0.5451 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.43351\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1532 - accuracy: 0.9439 - val_loss: 0.5302 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.43351\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1534 - accuracy: 0.9582 - val_loss: 0.5420 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.43351\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1475 - accuracy: 0.9516 - val_loss: 0.5437 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.43351\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1355 - accuracy: 0.9604 - val_loss: 0.5638 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.43351\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1320 - accuracy: 0.9692 - val_loss: 0.5597 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.43351\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1387 - accuracy: 0.9549 - val_loss: 0.5518 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.43351\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1321 - accuracy: 0.9604 - val_loss: 0.5353 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.43351\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1280 - accuracy: 0.9560 - val_loss: 0.5217 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.43351\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1194 - accuracy: 0.9692 - val_loss: 0.5315 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.43351\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1185 - accuracy: 0.9593 - val_loss: 0.5419 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.43351\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1169 - accuracy: 0.9725 - val_loss: 0.5214 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.43351\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1302 - accuracy: 0.9549 - val_loss: 0.5113 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.43351\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1197 - accuracy: 0.9703 - val_loss: 0.5601 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.43351\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1139 - accuracy: 0.9648 - val_loss: 0.5731 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.43351\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1118 - accuracy: 0.9714 - val_loss: 0.5954 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.43351\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1073 - accuracy: 0.9681 - val_loss: 0.5816 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.43351\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1087 - accuracy: 0.9747 - val_loss: 0.5716 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.43351\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0981 - accuracy: 0.9791 - val_loss: 0.5661 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.43351\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1152 - accuracy: 0.9670 - val_loss: 0.6016 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.43351\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1117 - accuracy: 0.9703 - val_loss: 0.6122 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.43351\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1138 - accuracy: 0.9648 - val_loss: 0.6284 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.43351\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1010 - accuracy: 0.9736 - val_loss: 0.6527 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.43351\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1034 - accuracy: 0.9681 - val_loss: 0.6203 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.43351\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1016 - accuracy: 0.9725 - val_loss: 0.5794 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.43351\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0938 - accuracy: 0.9769 - val_loss: 0.5853 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.43351\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0877 - accuracy: 0.9780 - val_loss: 0.5835 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.43351\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0767 - accuracy: 0.9802 - val_loss: 0.6183 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.43351\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0880 - accuracy: 0.9824 - val_loss: 0.6300 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.43351\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0807 - accuracy: 0.9791 - val_loss: 0.6199 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.43351\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0787 - accuracy: 0.9813 - val_loss: 0.6253 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.43351\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0709 - accuracy: 0.9824 - val_loss: 0.6149 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.43351\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0731 - accuracy: 0.9813 - val_loss: 0.6046 - val_accuracy: 0.8542\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.43351\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0729 - accuracy: 0.9813 - val_loss: 0.5994 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.43351\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0771 - accuracy: 0.9780 - val_loss: 0.5758 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.43351\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0733 - accuracy: 0.9769 - val_loss: 0.5891 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.43351\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0729 - accuracy: 0.9758 - val_loss: 0.6004 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.43351\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0734 - accuracy: 0.9813 - val_loss: 0.5833 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.43351\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0728 - accuracy: 0.9813 - val_loss: 0.6190 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.43351\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0641 - accuracy: 0.9857 - val_loss: 0.6449 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.43351\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0703 - accuracy: 0.9813 - val_loss: 0.6459 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.43351\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0610 - accuracy: 0.9846 - val_loss: 0.6045 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.43351\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0653 - accuracy: 0.9868 - val_loss: 0.5980 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.43351\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0684 - accuracy: 0.9846 - val_loss: 0.6061 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.43351\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0681 - accuracy: 0.9835 - val_loss: 0.6374 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.43351\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0705 - accuracy: 0.9846 - val_loss: 0.6714 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.43351\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6f060e1dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "import ast \n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "import numpy\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "histories = []\n",
    "preds = []\n",
    "\n",
    "\n",
    "checkpoint_filepath = './tmp/checkpoint_bahar_a5'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      monitor='val_loss',\n",
    "      verbose=1,\n",
    "      save_best_only=True)\n",
    "\n",
    "for batch in [16, 32, 64, 128]:\n",
    "    inputs = Input(shape=(len(X_train_embeddings[0]),))\n",
    "    model = Dense(64, activation='relu')(inputs)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = Dense(32, activation='tanh')(model)\n",
    "    model = Dropout(0.2)(model)\n",
    "    output = Dense(3, activation='softmax')(model)\n",
    "    model = Model(inputs, output)\n",
    "\n",
    "    opt = tensorflow.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    y_train_onehot = to_categorical(y_train, num_classes=3)\n",
    "    y_test_onehot = to_categorical(y_test, num_classes=3)\n",
    "    history = model.fit(\n",
    "        X_train_embeddings, y_train_onehot, \n",
    "        epochs=100, batch_size=batch,\n",
    "        validation_data=(X_test_embeddings, y_test_onehot),\n",
    "        callbacks=[model_checkpoint_callback], shuffle=True)\n",
    "    histories.append(history)\n",
    "\n",
    "# to get predictions in the format [[2.], [0.], [2.], ...]\n",
    "predictions = np.argmax(model.predict(X_test_embeddings), axis=-1).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "40804cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in code 300 ta model deep learning ro train mikone ba batch size hA va epoc haye mokhtalf va to motuni in model ro azin be bad estefade koni "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "060c5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model = keras.models.load_model('./tmp/checkpoint_bahar_a5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7ec835f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = './tmp/checkpoint_amin_a/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "730fcdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.88      0.88      0.88         8\n",
      "     Class 1       0.75      0.60      0.67        10\n",
      "     Class 2       0.91      0.97      0.94        30\n",
      "\n",
      "    accuracy                           0.88        48\n",
      "   macro avg       0.84      0.81      0.83        48\n",
      "weighted avg       0.87      0.88      0.87        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the trained model from the checkpoint\n",
    "model = keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Convert integer labels to one-hot encoded labels\n",
    "y_test_encoded = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# Predict labels for X_test_embeddings\n",
    "y_pred_encoded = model.predict(X_test_embeddings)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_encoded, axis=1)\n",
    "y_test = np.argmax(y_test_encoded, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "target_names = ['Class 0', 'Class 1', 'Class 2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "441b6a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6f020aaaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.83      0.62      0.71         8\n",
      "     Class 1       0.56      0.50      0.53        10\n",
      "     Class 2       0.85      0.93      0.89        30\n",
      "\n",
      "    accuracy                           0.79        48\n",
      "   macro avg       0.75      0.69      0.71        48\n",
      "weighted avg       0.78      0.79      0.78        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the trained model from the checkpoint\n",
    "model = keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Convert integer labels to one-hot encoded labels\n",
    "y_test_encoded = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# Predict labels for X_test_embeddings\n",
    "y_pred_encoded = model.predict(X_test_embeddings)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_encoded, axis=1)\n",
    "y_test = np.argmax(y_test_encoded, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "target_names = ['Class 0', 'Class 1', 'Class 2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b63bd77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2f221474c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.67      0.50      0.57         4\n",
      "     Class 1       0.87      0.93      0.90        14\n",
      "     Class 2       0.94      0.94      0.94        31\n",
      "\n",
      "    accuracy                           0.90        49\n",
      "   macro avg       0.82      0.79      0.80        49\n",
      "weighted avg       0.89      0.90      0.89        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the trained model from the checkpoint\n",
    "model = keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Convert integer labels to one-hot encoded labels\n",
    "y_test_encoded = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# Predict labels for X_test_embeddings\n",
    "y_pred_encoded = model.predict(X_test_embeddings)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_encoded, axis=1)\n",
    "y_test = np.argmax(y_test_encoded, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "target_names = ['Class 0', 'Class 1', 'Class 2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7c95e62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.77      0.91      0.83        11\n",
      "     Class 1       0.50      0.43      0.46         7\n",
      "     Class 2       0.90      0.87      0.89        31\n",
      "\n",
      "    accuracy                           0.82        49\n",
      "   macro avg       0.72      0.74      0.73        49\n",
      "weighted avg       0.81      0.82      0.81        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the trained model from the checkpoint\n",
    "model = keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Convert integer labels to one-hot encoded labels\n",
    "y_test_encoded = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# Predict labels for X_test_embeddings\n",
    "y_pred_encoded = model.predict(X_test_embeddings)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_encoded, axis=1)\n",
    "y_test = np.argmax(y_test_encoded, axis=1)\n",
    "\n",
    "# Generate a classification report\n",
    "target_names = ['Class 0', 'Class 1', 'Class 2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "519023ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sigm= [1 if pred_ > thresh else 0 for pred_ in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "86be3d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.70      0.74        10\n",
      "           1       0.67      0.50      0.57         8\n",
      "           2       0.91      1.00      0.95        31\n",
      "\n",
      "    accuracy                           0.86        49\n",
      "   macro avg       0.79      0.73      0.75        49\n",
      "weighted avg       0.84      0.86      0.85        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4786e3af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_sigm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/workdir/Bahar/temp/ipykernel_21833/1680181575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_sigm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_sigm' is not defined"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred_sigm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#sns.heatmap(confusion_matrix(pred_soft[:,1], y_test_label[:,1]), annot= True)\n",
    "sns.heatmap(confusion_matrix(y_test,pred_sigm),cmap=\"Blues\", annot= True)\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(histories[0].history['binary_accuracy'])\n",
    "plt.plot(histories[0].history['val_binary_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "display(plt.show())\n",
    "# summarize history for loss\n",
    "plt.plot(histories[0].history['loss'])\n",
    "plt.plot(histories[0].history['val_loss'])\n",
    "plt.title('model loss')            \n",
    "\n",
    "\n",
    "\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73975c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
